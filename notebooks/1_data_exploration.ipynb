{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89002a92",
   "metadata": {},
   "source": [
    "# Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a540fcf3",
   "metadata": {},
   "source": [
    "### 1.  Dataset Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de2574b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start loading 'Sentiment140' dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 100%|██████████| 81.4M/81.4M [00:06<00:00, 12.8MB/s]\n",
      "Generating train split: 100%|██████████| 1600000/1600000 [00:16<00:00, 94977.47 examples/s] \n",
      "Generating test split: 100%|██████████| 498/498 [00:00<00:00, 66881.09 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loading successful!\n",
      "\n",
      "Random sampling is being conducted, and 50,000 pieces of data are being selected...\n",
      "Save 50,000 pieces of data to: /Users/yaoyue/Desktop/CS/MSc Project/Project/uncertainty-sentiment-analysis/data/raw/sentiment140_50k.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating CSV from Arrow format: 100%|██████████| 50/50 [00:00<00:00, 157.80ba/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Task completed！A CSV file containing 50,000 pieces of data has been created.\n",
      "File name：'sentiment140_50k.csv'\n",
      "Storage location：/Users/yaoyue/Desktop/CS/MSc Project/Project/uncertainty-sentiment-analysis/data/raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "# 1.Path definition\n",
    "# The storage path of the dataset\n",
    "output_dir = \"../data/raw/\"\n",
    "output_filename = \"sentiment140_50k.csv\"\n",
    "output_filepath = os.path.join(output_dir, output_filename)\n",
    "\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Define the cache path used when downloading Hugging Face\n",
    "cache_directory = \"../data/raw/hf_cache/\"\n",
    "os.makedirs(cache_directory, exist_ok=True)\n",
    "\n",
    "try:\n",
    "    # 2.Load the dataset\n",
    "    print(\"Start loading 'Sentiment140' dataset...\")\n",
    "    dataset_name = \"stanfordnlp/sentiment140\"\n",
    "    dataset = load_dataset(dataset_name, cache_dir=cache_directory)\n",
    "    print(\"Dataset loading successful!\")\n",
    "\n",
    "    # 3.Data sampling\n",
    "    # Randomly shuffle the entire dataset and then sample the first 50,000 data entries.\n",
    "    print(\"\\nRandom sampling is being conducted, and 50,000 pieces of data are being selected...\")\n",
    "    sample_dataset = dataset['train'].shuffle(seed=42).select(range(50000))\n",
    "    \n",
    "    # 4.Generate the local original dataset\n",
    "    # Save the sampled data set as a single CSV file\n",
    "    print(f\"Save 50,000 pieces of data to: {os.path.abspath(output_filepath)}\")\n",
    "    sample_dataset.to_csv(output_filepath)\n",
    "    \n",
    "    print(\"\\nTask completed！A CSV file containing 50,000 pieces of data has been created.\")\n",
    "    print(f\"File name：'{output_filename}'\")\n",
    "    print(f\"Storage location：{os.path.abspath(output_dir)}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Making mistakes when processing data: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c3e5bd4",
   "metadata": {},
   "source": [
    "### 2.Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b71677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the locally downloaded dataset...\n",
      "Successfully loaded 50000 pieces of data.\n",
      "Data Preview:\n",
      "                                                text  \\\n",
      "0  why am i awake so early?  damn projects. super...   \n",
      "1  watching church online because I'd be half an ...   \n",
      "2                                         Hillsong!    \n",
      "3  is at Stafford Train Station and just watched ...   \n",
      "4           thanks everyone for the follow fridays!    \n",
      "\n",
      "                           date         user  sentiment     query  \n",
      "0  Sun Jun 07 07:43:33 PDT 2009  _stacey_rae          0  NO_QUERY  \n",
      "1  Sun May 31 06:16:45 PDT 2009     Trollyjd          0  NO_QUERY  \n",
      "2  Fri May 29 17:35:07 PDT 2009     ffaithyy          4  NO_QUERY  \n",
      "3  Fri Jun 19 23:28:43 PDT 2009   VCasambros          0  NO_QUERY  \n",
      "4  Fri Jun 05 17:59:44 PDT 2009   angela_woo          4  NO_QUERY  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 50000 entries, 0 to 49999\n",
      "Data columns (total 5 columns):\n",
      " #   Column     Non-Null Count  Dtype \n",
      "---  ------     --------------  ----- \n",
      " 0   text       50000 non-null  object\n",
      " 1   date       50000 non-null  object\n",
      " 2   user       50000 non-null  object\n",
      " 3   sentiment  50000 non-null  int64 \n",
      " 4   query      50000 non-null  object\n",
      "dtypes: int64(1), object(4)\n",
      "memory usage: 1.9+ MB\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "print(\"Loading the locally downloaded dataset...\")\n",
    "raw_data_path = '../data/raw/sentiment140_50k.csv'\n",
    "\n",
    "df = pd.read_csv(raw_data_path)\n",
    "\n",
    "print(f\"Successfully loaded {len(df)} pieces of data.\")\n",
    "print(\"Data Preview:\")\n",
    "print(df.head())\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9203411c",
   "metadata": {},
   "source": [
    "### 3.Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57438785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Clean the text and re-map the labels...\n",
      "Data cleaning and label remapping have been completed.\n",
      "Updated data preview:\n",
      "                                                text  \\\n",
      "0  why am i awake so early damn projects super ne...   \n",
      "1  watching church online because id be half an h...   \n",
      "2                                           hillsong   \n",
      "3  is at stafford train station and just watched ...   \n",
      "4             thanks everyone for the follow fridays   \n",
      "\n",
      "                           date         user  sentiment     query  \n",
      "0  Sun Jun 07 07:43:33 PDT 2009  _stacey_rae          0  NO_QUERY  \n",
      "1  Sun May 31 06:16:45 PDT 2009     Trollyjd          0  NO_QUERY  \n",
      "2  Fri May 29 17:35:07 PDT 2009     ffaithyy          2  NO_QUERY  \n",
      "3  Fri Jun 19 23:28:43 PDT 2009   VCasambros          0  NO_QUERY  \n",
      "4  Fri Jun 05 17:59:44 PDT 2009   angela_woo          2  NO_QUERY  \n",
      "\n",
      "Label distribution:\n",
      "sentiment\n",
      "0    25043\n",
      "2    24858\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "print(\"\\nClean the text and re-map the labels...\")\n",
    "\n",
    "# Text cleaning function\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Convert to lowercase\n",
    "    text = re.sub(r'http\\S+|www\\S+', '', text)  # Remove URL\n",
    "    text = re.sub(r'@\\w+', '', text)  # Remove username\n",
    "    text = text.replace('#', '')  # remove'#'\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove special characters\n",
    "    text = ' '.join(text.split())  # Remove unnecessary spaces\n",
    "    return text\n",
    "\n",
    "\n",
    "df['text'] = df['text'].apply(clean_text)\n",
    "\n",
    "# Re-map the labels\n",
    "label_map = {\n",
    "    0: 0,  # Negative\n",
    "    2: 1,  # Neutral\n",
    "    4: 2   # Positive\n",
    "}\n",
    "df['sentiment'] = df['sentiment'].map(label_map)\n",
    "df['sentiment'] = df['sentiment'].astype(int)\n",
    "\n",
    "# Delete rows that contain null values\n",
    "df.dropna(inplace=True)\n",
    "df = df[df['text'] != ''] # Keep the data as non-empty.\n",
    "\n",
    "print(\"Data cleaning and label remapping have been completed.\")\n",
    "print(\"Updated data preview:\")\n",
    "print(df.head())\n",
    "print(\"\\nLabel distribution:\")\n",
    "print(df['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3f8f66",
   "metadata": {},
   "source": [
    "### 4.Divide the dataset into training set, validation set and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b1e9dd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Divide the dataset into training set, validation set and test set...\n",
      "Size of Training set: 39920\n",
      "Size of Validation set: 4990\n",
      "Size of Test set: 4991\n",
      "All the processed datasets have been successfully saved!\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print(\"\\nDivide the dataset into training set, validation set and test set...\")\n",
    "\n",
    "# Divide the data into a 80% training set and a 20% test set.\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=df['sentiment'] # Keep the original proportion of label distribution\n",
    ")\n",
    "\n",
    "# Divide 20% of the temporary data into a 10% validation set and a 10% test set.\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,\n",
    "    random_state=42,\n",
    "    stratify=temp_df['sentiment']\n",
    ")\n",
    "\n",
    "print(f\"Size of Training set: {len(train_df)}\")\n",
    "print(f\"Size of Validation set: {len(val_df)}\")\n",
    "print(f\"Size of Test set: {len(test_df)}\")\n",
    "\n",
    "processed_dir = '../data/processed/'\n",
    "os.makedirs(processed_dir, exist_ok=True)\n",
    "\n",
    "train_df.to_csv(os.path.join(processed_dir, 'train.csv'), index=False)\n",
    "val_df.to_csv(os.path.join(processed_dir, 'validation.csv'), index=False)\n",
    "test_df.to_csv(os.path.join(processed_dir, 'test.csv'), index=False)\n",
    "\n",
    "print(\"All the processed datasets have been successfully saved!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
