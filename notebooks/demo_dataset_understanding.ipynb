{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "877a058e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模块导入成功！\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# 将src目录添加到Python的搜索路径中，这样我们才能导入我们自己写的模块\n",
    "# 这是一种常见的做法，用于让notebook能够找到项目中的.py文件\n",
    "# 我们需要返回上级目录'../'才能找到'src'\n",
    "module_path = os.path.abspath(os.path.join('..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "\n",
    "from src.dataset import SentimentDataset, create_data_loader\n",
    "from transformers import DistilBertTokenizer\n",
    "\n",
    "print(\"模块导入成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a80a389",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在创建 SentimentDataset 实例...\n",
      "实例创建成功！\n"
     ]
    }
   ],
   "source": [
    "# --- 定义基本参数 ---\n",
    "VALIDATION_FILE = '../data/processed/validation.csv'\n",
    "MAX_LEN = 128 # 序列最大长度，与dataset.py中保持一致\n",
    "TOKENIZER = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "# --- 创建 SentimentDataset 的实例 ---\n",
    "# 这会调用 SentimentDataset 类的 __init__ 方法\n",
    "print(\"正在创建 SentimentDataset 实例...\")\n",
    "val_dataset = SentimentDataset(\n",
    "    file_path=VALIDATION_FILE,\n",
    "    tokenizer=TOKENIZER,\n",
    "    max_len=MAX_LEN\n",
    ")\n",
    "print(\"实例创建成功！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2081f900",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. 调用 __len__ 方法:\n",
      "   └── 返回的数据集总长度为: 4990 条\n"
     ]
    }
   ],
   "source": [
    "# 调用 __len__ 方法\n",
    "dataset_length = len(val_dataset)\n",
    "\n",
    "print(f\"1. 调用 __len__ 方法:\")\n",
    "print(f\"   └── 返回的数据集总长度为: {dataset_length} 条\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b08d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2. 调用 __getitem__(9) 方法:\n",
      "   └── 返回的是一个字典，包含以下键: ['text', 'input_ids', 'attention_mask', 'labels']\n",
      "------------------------------\n",
      "   - 'text': (原始文本)\n",
      "     └── \"a man that has never lied to a woman has no respect for her feelings\"\n",
      "------------------------------\n",
      "   - 'labels': (情感标签，已经转换为PyTorch张量)\n",
      "     └── 2 (类型: torch.int64)\n",
      "------------------------------\n",
      "   - 'input_ids': (文本被分词并转换为数字ID后的张量)\n",
      "     └── 形状: torch.Size([128])\n",
      "     └── 内容: tensor([ 101, 1037, 2158, 2008, 2038, 2196, 9828, 2000, 1037, 2450, 2038, 2053,\n",
      "        4847, 2005, 2014, 5346,  102,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "------------------------------\n",
      "   - 'attention_mask': (注意力掩码，1代表真实词元，0代表填充词元)\n",
      "     └── 形状: torch.Size([128])\n",
      "     └── 内容: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "------------------------------\n",
      "   - 'input_ids' 反向转换为词元:\n",
      "     └── ['[CLS]', 'a', 'man', 'that', 'has', 'never', 'lied', 'to', 'a', 'woman', 'has', 'no', 'respect', 'for', 'her', 'feelings', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# 选择一个样本进行查看，例如第10个样本（索引为9）\n",
    "sample_index = 9\n",
    "\n",
    "# 调用 __getitem__ 方法\n",
    "single_item = val_dataset[sample_index]\n",
    "\n",
    "print(f\"2. 调用 __getitem__({sample_index}) 方法:\")\n",
    "print(f\"   └── 返回的是一个字典，包含以下键: {list(single_item.keys())}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 逐一打印字典中的内容\n",
    "print(f\"   - 'text': (原始文本)\")\n",
    "print(f\"     └── \\\"{single_item['text']}\\\"\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"   - 'labels': (情感标签，已经转换为PyTorch张量)\")\n",
    "print(f\"     └── {single_item['labels']} (类型: {single_item['labels'].dtype})\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"   - 'input_ids': (文本被分词并转换为数字ID后的张量)\")\n",
    "print(f\"     └── 形状: {single_item['input_ids'].shape}\")\n",
    "print(f\"     └── 内容: {single_item['input_ids']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "print(f\"   - 'attention_mask': (注意力掩码，1代表真实词元，0代表填充词元)\")\n",
    "print(f\"     └── 形状: {single_item['attention_mask'].shape}\")\n",
    "print(f\"     └── 内容: {single_item['attention_mask']}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "\n",
    "# 为了更直观地理解，我们将 input_ids 转换回词元\n",
    "tokens = TOKENIZER.convert_ids_to_tokens(single_item['input_ids'])\n",
    "print(\"   - 'input_ids' 反向转换为词元:\")\n",
    "print(f\"     └── {tokens}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf027148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3. 从 DataLoader 中获取一个批次的数据:\n",
      "   └── DataLoader 返回的也是一个字典，包含以下键: ['text', 'input_ids', 'attention_mask', 'labels']\n",
      "------------------------------\n",
      "   - 'input_ids' 的形状: torch.Size([4, 128])\n",
      "     └── [批大小, 最大长度] -> [4, 128]\n",
      "\n",
      "   - 'attention_mask' 的形状: torch.Size([4, 128])\n",
      "     └── [批大小, 最大长度] -> [4, 128]\n",
      "\n",
      "   - 'labels' 的形状: torch.Size([4])\n",
      "     └── [批大小] -> [4]\n"
     ]
    }
   ],
   "source": [
    "# 定义批大小\n",
    "BATCH_SIZE = 4\n",
    "\n",
    "# 调用我们写的辅助函数来创建 DataLoader\n",
    "val_data_loader = create_data_loader(\n",
    "    file_path=VALIDATION_FILE,\n",
    "    tokenizer=TOKENIZER,\n",
    "    max_len=MAX_LEN,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# 从 DataLoader 中取出一个批次的数据\n",
    "first_batch = next(iter(val_data_loader))\n",
    "\n",
    "\n",
    "print(\"3. 从 DataLoader 中获取一个批次的数据:\")\n",
    "print(f\"   └── DataLoader 返回的也是一个字典，包含以下键: {list(first_batch.keys())}\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# 打印这个批次中各个张量的形状\n",
    "# 注意，形状的第一维现在是 BATCH_SIZE (4)\n",
    "print(f\"   - 'input_ids' 的形状: {first_batch['input_ids'].shape}\")\n",
    "print(f\"     └── [批大小, 最大长度] -> [{BATCH_SIZE}, {MAX_LEN}]\")\n",
    "print(\"\")\n",
    "print(f\"   - 'attention_mask' 的形状: {first_batch['attention_mask'].shape}\")\n",
    "print(f\"     └── [批大小, 最大长度] -> [{BATCH_SIZE}, {MAX_LEN}]\")\n",
    "print(\"\")\n",
    "print(f\"   - 'labels' 的形状: {first_batch['labels'].shape}\")\n",
    "print(f\"     └── [批大小] -> [{BATCH_SIZE}]\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
